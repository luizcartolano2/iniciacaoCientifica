\section{PROPOSTA} \label{sec:proposta}
Nossa proposta de trabalho pode ser divida em etapas, que serão comentadas separadamente. O cronograma das etapas pode ser visto na Figura \ref{fig:cronograma}. Até o momento, já foram realizadas, como previsto no cronograma as seguintes atividades:

\subsection{Revisão bibliográfica}
A primeira parte do trabalho consistiu em um estudo bibliográfico de artigos científicos relacionados com os tópicos desse trabalho. A grande maioria dos artigos lidos, os mais significativos, ao meu ver, foram citados nesse trabalho na seção \ref{sec:trabalhos}. Os demais, ainda que tenham dados bons \emph{insights} para o desenvolvimento do projeto, eram, em sua grande maioria, de escopos muito diferentes para com o nosso.

\subsection{Definição da estratégia de coleta e mineração}
Os dados aqui obtidos são parte crucial para o projeto. Por isso, em um primeiro momento, dedicou-se os esforços a traçar uma estratégia adequada, definindo as fontes de informação e os espaços a serem monitorados. Para o primeiro, optou-se pelo \emph{Twitter} e o \emph{Google Maps}. Para a primeira fonte, decidiu-se a macro região de Campinas-SP como espaço a ser monitorado, enquanto que para o \emph{Maps} as avenidas escolhidas podem ser vistas na Figura \ref{fig:avenidas}. 

Primeiro, iremos introduzir a metodologia de trabalho para o \emph{Twitter}, usou-se um código em Python que realiza requisições por meio da API (Interface de Programação de Aplicativos) disponibilizada pela própria empresa. Esta é responsável por conectar o código ao servidor do Twitter, e produzir uma stream com os dados, então filtramos essa stream para armazenarmos somente os dados que nos interessam (nesse caso usamos as coordenadas da cidade de Campinas - SP como filtro da stream). Os dados filtrados são salvos em um arquivo ".json", onde cada tweet é um objeto do JSON. Uma vez obtido os dados, partimos para uma segunda etapa do projeto, na qual foi feito um pré-processamento dos dados obtidos pelo Twitter. Nesta etapa, primeiro realizamos uma "leitura" dos dados salvos no arquivo JSON, separando os tweets em uma espécie de dicionário, no qual cada lista de coordenadas obtidas estava associada ao dia no qual ela foi coletada. Uma vez separados em dias de coleta, os dados passaram por uma análise que buscava encontrar possíveis bots (aplicação de software concebido para simular ações humanas repetidas vezes de maneira padrão). Excluídos os bots, fez se um plot do gráfico de calor produzido pelas coordenadas para cada dia no qual a coleta foi realizada. O resultado desse mapa pode ser visto na Figura \ref{fig:heatmap}.

Para os dados coletados junto ao Google Maps também usamos um código em Python que realiza requisições por meio de uma API disponibilizada pela própria empresa. Nesta informamos coordenadas de pontos iniciais e finais (isto é, origem e destino) que nos interessam e temos, como retorno, um arquivo ".json" que nos informa, para cada requisição feita, o tempo de viagem entre os pontos informados. O trabalho com os dados foi parecido com o feito com os do Twitter. Primeiro, realizamos uma "leitura" dos dados salvos no arquivo JSON, separando-os em uma espécie de dicionário, no qual cada rota foi separada junto a informações importantes sobre elas, como distância, tempo de viagem e data. A partir desses dados foi criada uma página HTML com as informações de cada rota coletada, um exemplo dessa página pode ser vista na Figura \ref{fig:pagina}.

\subsection{Definição da estratégia de geração de contexto}
Fase atual de desenvolvimento do projeto, uma vez que já temos os scripts de coleta e mineração de dados funcionando. Para geração de contexto, concentraremos nossos esforços na criação de um grafo que representará a malha viária da cidade de Campinas-SP.

Para gerar o grafo, seguiremos os seguintes passos. Usando o \emph{OpenStreetMaps}, importaremos o mapa da cidade em um arquivo \emph{.osm}, este será usado para gerar um arquivo \emph{.net.xml}, a partir do simulador \emph{SUMO}, ambos os arquivos serão cruciais na esquematização do grafo. A partir do arquivo \emph{.osm} extrairemos os \emph{nós}, enquanto que as arestas serão obtidas do arquivo \emph{.net.xml}. Utilizaremos as informações das rotas sugeridas pelo \emph{Google Maps} para ponderar as arestas do grafo com a velocidade média (distância/tempo). Dessa forma, saberemos qual é o padrão de mobilidade nas regiões das arestas que serão estudadas, e será possível saber o impacto que algum evento anômalo ocasiona na mobilidade dessas áreas.

\subsection{Avaliação e validação da estratégia}
A avaliação e validação da estratégia deve acontecer uma vez que a ferramenta esteja totalmente funcional, isto é, visto que já possuímos material necessário para coletar e minerar os dados, uma ferramenta poderá ser disponibilizada após a criação do grafo. Uma vez que o serviço esteja disponível para o público em geral será possível colher \emph{feedbacks} a fim de validar a qualidade e funcionalidade do projeto.

\subsection{Escrita de relatório e documentação}
Ao fim do projeto pretendemos concentrar esforços na escrita de relatórios e na documentação do trabalho realizado até aqui, a fim de facilitar o uso do que foi desenvolvido por outras pessoas nos mais variados tipos de projetos.

Para o segundo semestre desse ano (\the\year), planeja-se o a criação e submissão de artigos em conferências de temas relacionados ao desenvolvido nesse projeto. Para que tal etapa se torne realidade, falta apenas a obtenção de resultados mais concretos, isto é, terminada a implementação do grafo representado a malha viária de Campinas-SP, teremos material suficiente para a submissão de tais artigos.
